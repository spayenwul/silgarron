# services/llm_service.py
import os
from typing import List
import google.generativeai as genai
from dotenv import load_dotenv
from typing import List, Optional
import json
from utils.logger import log_llm_trace
from utils.prompt_manager import load_and_format_prompt
from logic.constants import *

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")
MODEL_NAME = os.getenv("GEMINI_MODEL", "gemini-2.5-pro") 


try:
    if not API_KEY:
        raise ValueError("–ö–ª—é—á API Gemini –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª .env –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é GEMINI_API_KEY.")
    
    genai.configure(api_key=API_KEY)
    print(f"–°–µ—Ä–≤–∏—Å Gemini —É—Å–ø–µ—à–Ω–æ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: {MODEL_NAME}")

except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Gemini: {e}")


# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ API ---
def _send_prompt_to_gemini(request_package: dict) -> str:
    """
    –ï–¥–∏–Ω–∞—è, —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ª—é–±–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –≤ Gemini.
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç "–ø–∞–∫–µ—Ç –∑–∞–ø—Ä–æ—Å–∞", –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –µ–≥–æ –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π —Å–ª–µ–¥.
    """
    # 1. –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–∞–∫–µ—Ç–∞
    prompt = request_package.get("prompt", "")
    
    raw_response = ""
    # 2. –°–æ–∑–¥–∞–µ–º "—Å–∫–µ–ª–µ—Ç" –¥–ª—è –ª–æ–≥–∞, –∫–æ–ø–∏—Ä—É—è –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–∞–∫–µ—Ç–∞
    trace = request_package.copy()
    trace["error"] = None
    
    try:
        # 3. –í—ã–∑—ã–≤–∞–µ–º API
        model = genai.GenerativeModel(MODEL_NAME)
        print("...–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Gemini...")
        response = model.generate_content(prompt)
        raw_response = response.text.strip()
        
        trace["raw_response"] = raw_response
        return raw_response
        
    except Exception as e:
        # 4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ª—é–±—ã–µ –æ—à–∏–±–∫–∏
        error_message = f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê API Gemini: {e}"
        print(f"üî¥ {error_message}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º "–∞–≤–∞—Ä–∏–π–Ω—ã–π" JSON-–æ—Ç–≤–µ—Ç
        raw_response = """
        {
          "narrative": "–í –º–∏—Ä–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–æ–∏–∑–æ—à–µ–ª —Å–±–æ–π. –ù–∞ –º–≥–Ω–æ–≤–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ—Å–Ω—É–ª–∞, –Ω–µ –≤ —Å–∏–ª–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–∞—à–µ –¥–µ–π—Å—Ç–≤–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
          "state_changes": {}
        }
        """
        trace["error"] = error_message
        trace["raw_response"] = raw_response
        return raw_response
        
    finally:
        # 5. –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–µ–≤–∞–∂–Ω–æ, —É—Å–ø–µ—à–Ω—ã–π –æ–Ω –∏–ª–∏ –Ω–µ—Ç
        log_llm_trace(trace)

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–ø–æ–∫–∞ —á—Ç–æ) ---
def generate_location_description(tags: List[str], context: Optional[List[str]] = None) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ù–û–í–û–ô –ª–æ–∫–∞—Ü–∏–∏.
    –≠—Ç–æ –æ–¥–Ω–∞ –∏–∑ –Ω–µ–º–Ω–æ–≥–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä–∞—è —Å–∞–º–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç.
    """
    tags_str = ", ".join(tags)

    context_block = ""
    if context:
        context_items_str = "\n".join(f"- {item}" for item in context)
        context_block = (
            "\n–£—á—Ç–∏ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –º–∏—Ä–∞ (—ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª—É—Ö, —Ñ–∞–∫—Ç –∏–ª–∏ –ø—Ä–æ—à–ª–æ–µ —Å–æ–±—ã—Ç–∏–µ):"
            f"\n{context_items_str}"
        )

    prompt = load_and_format_prompt(
        'location_description',
        tags_str=tags_str,
        context_block=context_block
    )
    
    # –°–æ–±–∏—Ä–∞–µ–º "–ø–∞–∫–µ—Ç" –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    llm_request = {
        "prompt": prompt,
        "prompt_template_name": "location_description",
        "game_state": "GENERATION" # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –ª–æ–≥–∞
    }
    
    return _send_prompt_to_gemini(llm_request)